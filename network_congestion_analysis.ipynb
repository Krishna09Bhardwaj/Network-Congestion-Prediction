{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Congestion Analysis with Machine Learning\n",
    "\n",
    "This notebook analyzes network congestion data using various machine learning techniques. The workflow includes:\n",
    "\n",
    "1. Data Loading and Preprocessing\n",
    "2. Exploratory Data Analysis\n",
    "3. Feature Engineering & Selection\n",
    "4. Model Training (Multiple Algorithms)\n",
    "5. Model Evaluation\n",
    "6. Results Visualization\n",
    "7. Feature Importance Analysis\n",
    "8. Prediction Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from datetime import datetime\n",
    "\n",
    "# For preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, StratifiedKFold\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Machine Learning models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "# For evaluation\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc, precision_recall_curve\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# For feature importance\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Set display options for pandas\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 20)\n",
    "\n",
    "# Set visualization style\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the network congestion dataset\n",
    "data = pd.read_csv('Network Congestion Dataset.csv')\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(f\"Dataset Shape: {data.shape}\")\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data information\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "missing_values = data.isnull().sum()\n",
    "print(\"Missing values per column:\")\n",
    "print(missing_values[missing_values > 0] if any(missing_values > 0) else \"No missing values found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistical summary\n",
    "data.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing and Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert timestamp to datetime\n",
    "data['Timestamp'] = pd.to_datetime(data['Timestamp'])\n",
    "\n",
    "# Extract additional time-based features\n",
    "data['Hour'] = data['Timestamp'].dt.hour\n",
    "data['Day'] = data['Timestamp'].dt.day\n",
    "data['DayOfWeek'] = data['Timestamp'].dt.dayofweek\n",
    "data['IsWeekend'] = data['DayOfWeek'].apply(lambda x: 1 if x >= 5 else 0)\n",
    "\n",
    "# Display the updated dataset with new features\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check unique values in categorical columns\n",
    "categorical_cols = ['Source_Node', 'Destination_Node', 'Admin_Contact', 'Region_Code']\n",
    "for col in categorical_cols:\n",
    "    print(f\"\\nUnique values in {col}:\")\n",
    "    print(data[col].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create connection pairs for analysis\n",
    "data['Connection'] = data['Source_Node'] + ' → ' + data['Destination_Node']\n",
    "\n",
    "# Check outliers in numerical columns\n",
    "numerical_cols = ['Packet_Loss_Rate', 'Average_Latency_ms', 'Node_Betweenness_Centrality', \n",
    "                  'Traffic_Volume_MBps', 'Link_Stability_Score']\n",
    "\n",
    "# Boxplot for numerical features to identify outliers\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i, col in enumerate(numerical_cols):\n",
    "    plt.subplot(3, 2, i+1)\n",
    "    sns.boxplot(x=data[col])\n",
    "    plt.title(f'Boxplot of {col}')\n",
    "    plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to identify outliers using IQR method\n",
    "def identify_outliers(data, column):\n",
    "    Q1 = data[column].quantile(0.25)\n",
    "    Q3 = data[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers = data[(data[column] < lower_bound) | (data[column] > upper_bound)]\n",
    "    return outliers, lower_bound, upper_bound, len(outliers)\n",
    "\n",
    "# Check for outliers in each numerical column\n",
    "for col in numerical_cols:\n",
    "    outliers, lower, upper, count = identify_outliers(data, col)\n",
    "    print(f\"\\nOutliers in {col}:\")\n",
    "    print(f\"Lower bound: {lower:.4f}, Upper bound: {upper:.4f}\")\n",
    "    print(f\"Number of outliers: {count} ({(count/len(data))*100:.2f}% of data)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to handle outliers using capping\n",
    "def cap_outliers(data, column):\n",
    "    Q1 = data[column].quantile(0.25)\n",
    "    Q3 = data[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    data[column] = np.where(data[column] < lower_bound, lower_bound, data[column])\n",
    "    data[column] = np.where(data[column] > upper_bound, upper_bound, data[column])\n",
    "    return data\n",
    "\n",
    "# Handle outliers for each numerical column\n",
    "data_processed = data.copy()\n",
    "for col in numerical_cols:\n",
    "    data_processed = cap_outliers(data_processed, col)\n",
    "\n",
    "# Verify outliers are handled\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i, col in enumerate(numerical_cols):\n",
    "    plt.subplot(3, 2, i+1)\n",
    "    sns.boxplot(x=data_processed[col])\n",
    "    plt.title(f'Boxplot of {col} (After Handling Outliers)')\n",
    "    plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a congestion level target variable based on combined factors\n",
    "# We'll use Packet_Loss_Rate and Average_Latency_ms as our main indicators\n",
    "\n",
    "# Normalize the factors for weighted scoring\n",
    "loss_norm = (data_processed['Packet_Loss_Rate'] - data_processed['Packet_Loss_Rate'].min()) / \\\n",
    "            (data_processed['Packet_Loss_Rate'].max() - data_processed['Packet_Loss_Rate'].min())\n",
    "\n",
    "latency_norm = (data_processed['Average_Latency_ms'] - data_processed['Average_Latency_ms'].min()) / \\\n",
    "               (data_processed['Average_Latency_ms'].max() - data_processed['Average_Latency_ms'].min())\n",
    "\n",
    "# Combined congestion score (weighted average)\n",
    "data_processed['Congestion_Score'] = 0.6 * loss_norm + 0.4 * latency_norm\n",
    "\n",
    "# Create congestion level categories (Low, Medium, High)\n",
    "data_processed['Congestion_Level'] = pd.qcut(data_processed['Congestion_Score'], \n",
    "                                             q=[0, 0.33, 0.67, 1.0], \n",
    "                                             labels=['Low', 'Medium', 'High'])\n",
    "\n",
    "# Display the distribution of congestion levels\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(x='Congestion_Level', data=data_processed)\n",
    "plt.title('Distribution of Network Congestion Levels')\n",
    "plt.xlabel('Congestion Level')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "# Verify the resulting dataset\n",
    "data_processed[['Packet_Loss_Rate', 'Average_Latency_ms', 'Congestion_Score', 'Congestion_Level']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of numerical features\n",
    "plt.figure(figsize=(20, 15))\n",
    "for i, col in enumerate(numerical_cols):\n",
    "    plt.subplot(3, 2, i+1)\n",
    "    sns.histplot(data_processed[col], kde=True)\n",
    "    plt.title(f'Distribution of {col}')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix for numerical features\n",
    "numerical_data = data_processed[numerical_cols + ['Congestion_Score']]\n",
    "correlation_matrix = numerical_data.corr()\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n",
    "plt.title('Correlation Matrix of Numerical Features')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive correlation heatmap with Plotly\n",
    "fig = px.imshow(correlation_matrix, \n",
    "                text_auto='.2f',\n",
    "                color_continuous_scale='RdBu_r',\n",
    "                title='Correlation Matrix of Numerical Features')\n",
    "fig.update_layout(width=800, height=800)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hourly patterns in congestion\n",
    "hourly_congestion = data_processed.groupby('Hour')['Congestion_Score'].mean().reset_index()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(hourly_congestion['Hour'], hourly_congestion['Congestion_Score'], marker='o', linestyle='-')\n",
    "plt.title('Average Congestion Score by Hour of Day')\n",
    "plt.xlabel('Hour of Day')\n",
    "plt.ylabel('Average Congestion Score')\n",
    "plt.xticks(range(0, 24))\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive hourly pattern with Plotly\n",
    "fig = px.line(hourly_congestion, x='Hour', y='Congestion_Score', markers=True,\n",
    "              title='Average Congestion Score by Hour of Day')\n",
    "fig.update_layout(xaxis_title='Hour of Day', \n",
    "                  yaxis_title='Average Congestion Score',\n",
    "                  xaxis=dict(tickmode='linear', tick0=0, dtick=1))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze congestion by day of week\n",
    "day_mapping = {0: 'Monday', 1: 'Tuesday', 2: 'Wednesday', 3: 'Thursday', \n",
    "               4: 'Friday', 5: 'Saturday', 6: 'Sunday'}\n",
    "data_processed['DayName'] = data_processed['DayOfWeek'].map(day_mapping)\n",
    "\n",
    "daily_congestion = data_processed.groupby('DayName')['Congestion_Score'].mean().reindex(\n",
    "    ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    ").reset_index()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='DayName', y='Congestion_Score', data=daily_congestion)\n",
    "plt.title('Average Congestion Score by Day of Week')\n",
    "plt.xlabel('Day of Week')\n",
    "plt.ylabel('Average Congestion Score')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regional analysis\n",
    "region_congestion = data_processed.groupby('Region_Code')['Congestion_Score'].mean().sort_values(ascending=False).reset_index()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='Region_Code', y='Congestion_Score', data=region_congestion)\n",
    "plt.title('Average Congestion Score by Region')\n",
    "plt.xlabel('Region')\n",
    "plt.ylabel('Average Congestion Score')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connection pair analysis\n",
    "connection_congestion = data_processed.groupby('Connection')['Congestion_Score'].mean().sort_values(ascending=False).reset_index()\n",
    "top_10_congested = connection_congestion.head(10)\n",
    "\n",
    "plt.figure(figsize=(14, 7))\n",
    "sns.barplot(x='Congestion_Score', y='Connection', data=top_10_congested)\n",
    "plt.title('Top 10 Most Congested Connection Pairs')\n",
    "plt.xlabel('Average Congestion Score')\n",
    "plt.ylabel('Connection Pair')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot to examine relationship between key variables\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.subplot(2, 2, 1)\n",
    "sns.scatterplot(x='Packet_Loss_Rate', y='Average_Latency_ms', hue='Congestion_Level', data=data_processed)\n",
    "plt.title('Packet Loss Rate vs. Average Latency')\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "sns.scatterplot(x='Traffic_Volume_MBps', y='Packet_Loss_Rate', hue='Congestion_Level', data=data_processed)\n",
    "plt.title('Traffic Volume vs. Packet Loss Rate')\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "sns.scatterplot(x='Node_Betweenness_Centrality', y='Packet_Loss_Rate', hue='Congestion_Level', data=data_processed)\n",
    "plt.title('Node Betweenness Centrality vs. Packet Loss Rate')\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "sns.scatterplot(x='Link_Stability_Score', y='Packet_Loss_Rate', hue='Congestion_Level', data=data_processed)\n",
    "plt.title('Link Stability Score vs. Packet Loss Rate')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive 3D scatter plot to visualize multiple dimensions\n",
    "fig = px.scatter_3d(data_processed, x='Packet_Loss_Rate', y='Average_Latency_ms', z='Traffic_Volume_MBps',\n",
    "                  color='Congestion_Level', opacity=0.7,\n",
    "                  title='3D Visualization of Network Congestion Factors')\n",
    "fig.update_layout(scene=dict(\n",
    "    xaxis_title='Packet Loss Rate',\n",
    "    yaxis_title='Average Latency (ms)',\n",
    "    zaxis_title='Traffic Volume (MBps)'),\n",
    "    width=900, height=700)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pair plot for key numerical features\n",
    "plt.figure(figsize=(15, 12))\n",
    "sns.pairplot(data_processed[numerical_cols + ['Congestion_Level']], hue='Congestion_Level', height=2.5)\n",
    "plt.suptitle('Pair Plot of Key Network Metrics', y=1.02, fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering & Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create interaction features\n",
    "data_processed['Loss_Latency_Interaction'] = data_processed['Packet_Loss_Rate'] * data_processed['Average_Latency_ms']\n",
    "data_processed['Traffic_Stability_Ratio'] = data_processed['Traffic_Volume_MBps'] / data_processed['Link_Stability_Score']\n",
    "data_processed['Centrality_Loss_Interaction'] = data_processed['Node_Betweenness_Centrality'] * data_processed['Packet_Loss_Rate']\n",
    "\n",
    "# Feature scaling for numerical columns\n",
    "numerical_cols_extended = numerical_cols + ['Loss_Latency_Interaction', 'Traffic_Stability_Ratio', 'Centrality_Loss_Interaction', \n",
    "                                           'Hour', 'Day', 'DayOfWeek']\n",
    "\n",
    "# Prepare data for modeling\n",
    "# Convert categorical 'Congestion_Level' to numeric for ML models\n",
    "level_mapping = {'Low': 0, 'Medium': 1, 'High': 2}\n",
    "data_processed['Congestion_Level_Numeric'] = data_processed['Congestion_Level'].map(level_mapping)\n",
    "\n",
    "# Select features and target for modeling\n",
    "X = data_processed[numerical_cols_extended]\n",
    "y = data_processed['Congestion_Level_Numeric']\n",
    "\n",
    "# Display the new features\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for correlation in extended features\n",
    "correlation_extended = X.corr()\n",
    "\n",
    "plt.figure(figsize=(16, 14))\n",
    "sns.heatmap(correlation_extended, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n",
    "plt.title('Correlation Matrix of Extended Features')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find highly correlated features (above 0.8)\n",
    "def get_highly_correlated_pairs(corr_matrix, threshold=0.8):\n",
    "    corr_pairs = []\n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i):\n",
    "            if abs(corr_matrix.iloc[i, j]) > threshold:\n",
    "                corr_pairs.append((corr_matrix.columns[i], corr_matrix.columns[j], corr_matrix.iloc[i, j]))\n",
    "    return corr_pairs\n",
    "\n",
    "high_corr_pairs = get_highly_correlated_pairs(correlation_extended, threshold=0.8)\n",
    "print(\"Highly correlated feature pairs:\")\n",
    "for feat1, feat2, corr in high_corr_pairs:\n",
    "    print(f\"{feat1} - {feat2}: {corr:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove highly correlated features to reduce multicollinearity\n",
    "# Based on the correlation analysis and feature importance\n",
    "features_to_drop = []\n",
    "for feat1, feat2, _ in high_corr_pairs:\n",
    "    # Strategy: Keep the original feature, drop the derived one\n",
    "    if feat1 in ['Loss_Latency_Interaction', 'Traffic_Stability_Ratio', 'Centrality_Loss_Interaction']:\n",
    "        features_to_drop.append(feat1)\n",
    "    elif feat2 in ['Loss_Latency_Interaction', 'Traffic_Stability_Ratio', 'Centrality_Loss_Interaction']:\n",
    "        features_to_drop.append(feat2)\n",
    "    # If both are original, drop the one with less correlation to target\n",
    "    else:\n",
    "        corr1 = abs(data_processed[feat1].corr(data_processed['Congestion_Level_Numeric']))\n",
    "        corr2 = abs(data_processed[feat2].corr(data_processed['Congestion_Level_Numeric']))\n",
    "        features_to_drop.append(feat1 if corr1 < corr2 else feat2)\n",
    "\n",
    "# Remove duplicates from the list\n",
    "features_to_drop = list(set(features_to_drop))\n",
    "print(f\"Features to drop due to high correlation: {features_to_drop}\")\n",
    "\n",
    "# Remove the correlated features\n",
    "X_reduced = X.drop(columns=features_to_drop)\n",
    "print(f\"Reduced feature set shape: {X_reduced.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_reduced, y, test_size=0.25, random_state=42, stratify=y)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Training set shape: {X_train_scaled.shape}\")\n",
    "print(f\"Testing set shape: {X_test_scaled.shape}\")\n",
    "print(f\"Class distribution in training set:\\n{pd.Series(y_train).value_counts(normalize=True)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Training & Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models to be evaluated\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "    'K-Nearest Neighbors': KNeighborsClassifier(),\n",
    "    'Support Vector Machine': SVC(probability=True, random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(random_state=42),\n",
    "    'XGBoost': xgb.XGBClassifier(random_state=42),\n",
    "    'LightGBM': lgb.LGBMClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "# Function to evaluate models using cross-validation\n",
    "def evaluate_models(models, X, y, cv=5):\n",
    "    results = {}\n",
    "    for name, model in models.items():\n",
    "        print(f\"\\nEvaluating {name}...\")\n",
    "        \n",
    "        # Cross-validation\n",
    "        cv_scores = cross_val_score(model, X, y, cv=cv, scoring='accuracy')\n",
    "        results[name] = {\n",
    "            'cv_accuracy_mean': cv_scores.mean(),\n",
    "            'cv_accuracy_std': cv_scores.std()\n",
    "        }\n",
    "        \n",
    "        print(f\"Cross-validation accuracy: {cv_scores.mean():.4f} ± {cv_scores.std():.4f}\")\n",
    "        \n",
    "        # Train on the full training set\n",
    "        model.fit(X, y)\n",
    "        \n",
    "    return results\n",
    "\n",
    "# Evaluate all models using cross-validation\n",
    "cv_results = evaluate_models(models, X_train_scaled, y_train, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize cross-validation results\n",
    "cv_means = [result['cv_accuracy_mean'] for result in cv_results.values()]\n",
    "cv_stds = [result['cv_accuracy_std'] for result in cv_results.values()]\n",
    "model_names = list(cv_results.keys())\n",
    "\n",
    "# Sort results by mean accuracy\n",
    "sorted_indices = np.argsort(cv_means)[::-1]  # Descending order\n",
    "cv_means = [cv_means[i] for i in sorted_indices]\n",
    "cv_stds = [cv_stds[i] for i in sorted_indices]\n",
    "model_names = [model_names[i] for i in sorted_indices]\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.barh(model_names, cv_means, xerr=cv_stds, capsize=5, alpha=0.7, color='skyblue')\n",
    "plt.xlabel('Mean Cross-Validation Accuracy')\n",
    "plt.ylabel('Model')\n",
    "plt.title('Cross-Validation Accuracy Comparison')\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "plt.xlim(min(cv_means) - 0.05, 1.0)\n",
    "\n",
    "# Add text annotations for mean accuracy values\n",
    "for i, mean in enumerate(cv_means):\n",
    "    plt.text(mean + 0.01, i, f\"{mean:.4f}\", va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the top-performing models\n",
    "top_models = {\n",
    "    name: models[name] for name in model_names[:3]  # Select top 3 models\n",
    "}\n",
    "\n",
    "print(\"Top performing models for hyperparameter tuning:\")\n",
    "for name in top_models.keys():\n",
    "    print(f\"- {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning for the top models\n",
    "def hyperparameter_tuning(model_name, model, X_train, y_train):\n",
    "    print(f\"\\nTuning hyperparameters for {model_name}...\")\n",
    "    \n",
    "    param_grid = {}\n",
    "    \n",
    "    if model_name == 'Random Forest':\n",
    "        param_grid = {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'max_depth': [None, 10, 20, 30],\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "            'min_samples_leaf': [1, 2, 4]\n",
    "        }\n",
    "    elif model_name == 'XGBoost':\n",
    "        param_grid = {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'max_depth': [3, 5, 7],\n",
    "            'learning_rate': [0.01, 0.1, 0.2],\n",
    "            'subsample': [0.8, 0.9, 1.0]\n",
    "        }\n",
    "    elif model_name == 'LightGBM':\n",
    "        param_grid = {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'max_depth': [3, 5, 7],\n",
    "            'learning_rate': [0.01, 0.1, 0.2],\n",
    "            'num_leaves': [31, 50, 70]\n",
    "        }\n",
    "    elif model_name == 'Gradient Boosting':\n",
    "        param_grid = {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'max_depth': [3, 5, 7],\n",
    "            'learning_rate': [0.01, 0.1, 0.2],\n",
    "            'min_samples_split': [2, 5, 10]\n",
    "        }\n",
    "    elif model_name == 'Support Vector Machine':\n",
    "        param_grid = {\n",
    "            'C': [0.1, 1, 10, 100],\n",
    "            'gamma': ['scale', 'auto', 0.1, 0.01],\n",
    "            'kernel': ['rbf', 'poly', 'sigmoid']\n",
    "        }\n",
    "    elif model_name == 'K-Nearest Neighbors':\n",
    "        param_grid = {\n",
    "            'n_neighbors': [3, 5, 7, 9, 11],\n",
    "            'weights': ['uniform', 'distance'],\n",
    "            'metric': ['euclidean', 'manhattan', 'minkowski']\n",
    "        }\n",
    "    else:  # Logistic Regression\n",
    "        param_grid = {\n",
    "            'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "            'solver': ['liblinear', 'saga'],\n",
    "            'penalty': ['l1', 'l2']\n",
    "        }\n",
    "    \n",
    "    # Use stratified k-fold cross-validation\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Create grid search\n",
    "    grid_search = GridSearchCV(model, param_grid, cv=cv, scoring='accuracy', n_jobs=-1, verbose=1)\n",
    "    \n",
    "    # Fit grid search\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Get best parameters and score\n",
    "    best_params = grid_search.best_params_\n",
    "    best_score = grid_search.best_score_\n",
    "    \n",
    "    print(f\"Best parameters for {model_name}: {best_params}\")\n",
    "    print(f\"Best cross-validation accuracy: {best_score:.4f}\")\n",
    "    \n",
    "    # Return best model\n",
    "    return grid_search.best_estimator_, best_params, best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune hyperparameters for top models\n",
    "tuned_models = {}\n",
    "for name, model in top_models.items():\n",
    "    best_model, best_params, best_score = hyperparameter_tuning(name, model, X_train_scaled, y_train)\n",
    "    tuned_models[name] = {\n",
    "        'model': best_model,\n",
    "        'params': best_params,\n",
    "        'cv_score': best_score\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation & Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the tuned models on the test set\n",
    "def evaluate_on_test_set(tuned_models, X_test, y_test):\n",
    "    results = {}\n",
    "    \n",
    "    for name, model_info in tuned_models.items():\n",
    "        model = model_info['model']\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_pred_proba = model.predict_proba(X_test)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred, average='weighted')\n",
    "        recall = recall_score(y_test, y_pred, average='weighted')\n",
    "        f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "        conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "        \n",
    "        # Store results\n",
    "        results[name] = {\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1,\n",
    "            'confusion_matrix': conf_matrix,\n",
    "            'predictions': y_pred,\n",
    "            'probabilities': y_pred_proba\n",
    "        }\n",
    "        \n",
    "        # Print evaluation results\n",
    "        print(f\"\\nEvaluation results for {name}:\")\n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"Precision: {precision:.4f}\")\n",
    "        print(f\"Recall: {recall:.4f}\")\n",
    "        print(f\"F1 Score: {f1:.4f}\")\n",
    "        print(\"\\nConfusion Matrix:\")\n",
    "        print(conf_matrix)\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(y_test, y_pred, target_names=['Low', 'Medium', 'High']))\n",
    "        \n",
    "    return results\n",
    "\n",
    "# Evaluate models on test set\n",
    "test_results = evaluate_on_test_set(tuned_models, X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize confusion matrices for each model\n",
    "def plot_confusion_matrix(conf_matrix, model_name):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['Low', 'Medium', 'High'],\n",
    "                yticklabels=['Low', 'Medium', 'High'])\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.title(f'Confusion Matrix - {model_name}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot confusion matrix for each model\n",
    "for name, result in test_results.items():\n",
    "    plot_confusion_matrix(result['confusion_matrix'], name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare model performances\n",
    "metrics = ['accuracy', 'precision', 'recall', 'f1_score']\n",
    "model_comparison = pd.DataFrame(index=metrics, columns=test_results.keys())\n",
    "\n",
    "for name, result in test_results.items():\n",
    "    for metric in metrics:\n",
    "        model_comparison.loc[metric, name] = result[metric]\n",
    "\n",
    "# Display model comparison table\n",
    "model_comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model comparison\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "# Create a grouped bar chart for model comparison\n",
    "bar_width = 0.2\n",
    "index = np.arange(len(test_results.keys()))\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    plt.bar(index + i*bar_width, model_comparison.loc[metric], bar_width, \n",
    "            label=metric.capitalize())\n",
    "\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Model Performance Comparison')\n",
    "plt.xticks(index + bar_width*1.5, test_results.keys(), rotation=45)\n",
    "plt.legend()\n",
    "plt.ylim(0.5, 1.0)  # Adjust as needed for your results\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive model comparison with Plotly\n",
    "fig = go.Figure()\n",
    "\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    fig.add_trace(go.Bar(\n",
    "        x=list(test_results.keys()),\n",
    "        y=model_comparison.loc[metric],\n",
    "        name=metric.capitalize(),\n",
    "        marker_color=colors[i]\n",
    "    ))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Model Performance Comparison',\n",
    "    xaxis_title='Model',\n",
    "    yaxis_title='Score',\n",
    "    yaxis=dict(range=[0.5, 1.0]),  # Adjust as needed\n",
    "    barmode='group',\n",
    "    width=900,\n",
    "    height=600\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC curves for each model (for multi-class, we'll use One-vs-Rest approach)\n",
    "def plot_roc_curves(tuned_models, X_test, y_test):\n",
    "    # Create binary labels for each class\n",
    "    n_classes = 3  # Low, Medium, High\n",
    "    y_test_bin = np.zeros((len(y_test), n_classes))\n",
    "    for i in range(n_classes):\n",
    "        y_test_bin[:, i] = (y_test == i).astype(int)\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    for name, model_info in tuned_models.items():\n",
    "        model = model_info['model']\n",
    "        y_pred_proba = model.predict_proba(X_test)\n",
    "        \n",
    "        # Calculate ROC curve and AUC for each class\n",
    "        fpr = dict()\n",
    "        tpr = dict()\n",
    "        roc_auc = dict()\n",
    "        \n",
    "        for i in range(n_classes):\n",
    "            fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_pred_proba[:, i])\n",
    "            roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "        \n",
    "        # Compute micro-average ROC curve and AUC\n",
    "        fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test_bin.ravel(), np.concatenate([y_pred_proba[:, i].reshape(-1, 1) for i in range(n_classes)], axis=1).ravel())\n",
    "        roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "        \n",
    "        # Plot micro-average ROC curve\n",
    "        plt.plot(fpr[\"micro\"], tpr[\"micro\"], label=f'{name} (AUC = {roc_auc[\"micro\"]:.4f})')\n",
    "    \n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curves')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot ROC curves\n",
    "plot_roc_curves(tuned_models, X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance from the best model\n",
    "def analyze_feature_importance(model_info, feature_names):\n",
    "    model = model_info['model']\n",
    "    feature_importance = None\n",
    "    \n",
    "    # Extract feature importance based on model type\n",
    "    if hasattr(model, 'feature_importances_'):  # Tree-based models\n",
    "        feature_importance = model.feature_importances_\n",
    "    elif hasattr(model, 'coef_'):  # Linear models\n",
    "        feature_importance = np.abs(model.coef_).mean(axis=0) if model.coef_.ndim > 1 else np.abs(model.coef_)\n",
    "    else:  # Use permutation importance\n",
    "        perm_importance = permutation_importance(model, X_test_scaled, y_test, n_repeats=10, random_state=42)\n",
    "        feature_importance = perm_importance.importances_mean\n",
    "    \n",
    "    # Create DataFrame for feature importance\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': feature_importance\n",
    "    })\n",
    "    \n",
    "    # Sort by importance\n",
    "    importance_df = importance_df.sort_values('Importance', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    return importance_df\n",
    "\n",
    "# Get the best model based on F1 score\n",
    "best_model_name = model_comparison.loc['f1_score'].idxmax()\n",
    "print(f\"Best model based on F1 score: {best_model_name}\")\n",
    "\n",
    "# Analyze feature importance\n",
    "feature_names = X_reduced.columns\n",
    "importance_df = analyze_feature_importance(tuned_models[best_model_name], feature_names)\n",
    "\n",
    "# Display feature importance\n",
    "print(\"\\nFeature Importance:\")\n",
    "importance_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature importance\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='Importance', y='Feature', data=importance_df.head(10), palette='viridis')\n",
    "plt.title(f'Top 10 Feature Importance ({best_model_name})')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive feature importance visualization with Plotly\n",
    "fig = px.bar(importance_df.head(10),\n",
    "            x='Importance', y='Feature',\n",
    "            orientation='h',\n",
    "            title=f'Top 10 Feature Importance ({best_model_name})',\n",
    "            color='Importance',\n",
    "            color_continuous_scale='viridis')\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis_title='Importance',\n",
    "    yaxis_title='Feature',\n",
    "    height=600,\n",
    "    width=900\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Prediction Analysis & Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best model for prediction analysis\n",
    "best_model = tuned_models[best_model_name]['model']\n",
    "y_pred = test_results[best_model_name]['predictions']\n",
    "y_pred_proba = test_results[best_model_name]['probabilities']\n",
    "\n",
    "# Create a DataFrame with actual and predicted values\n",
    "prediction_df = pd.DataFrame({\n",
    "    'Actual': y_test,\n",
    "    'Predicted': y_pred,\n",
    "    'Prob_Low': y_pred_proba[:, 0],\n",
    "    'Prob_Medium': y_pred_proba[:, 1],\n",
    "    'Prob_High': y_pred_proba[:, 2]\n",
    "})\n",
    "\n",
    "# Add a column to indicate correct/incorrect predictions\n",
    "prediction_df['Correct'] = prediction_df['Actual'] == prediction_df['Predicted']\n",
    "\n",
    "# Display sample of predictions\n",
    "prediction_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize prediction distribution\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.countplot(x='Actual', hue='Predicted', data=prediction_df,\n",
    "              palette='viridis', alpha=0.7)\n",
    "plt.title('Actual vs Predicted Congestion Levels')\n",
    "plt.xlabel('Actual Congestion Level')\n",
    "plt.ylabel('Count')\n",
    "plt.legend(title='Predicted', loc='upper right')\n",
    "plt.xticks([0, 1, 2], ['Low', 'Medium', 'High'])\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.countplot(x='Correct', data=prediction_df, palette=['red', 'green'])\n",
    "plt.title('Prediction Accuracy')\n",
    "plt.xlabel('Prediction Correct')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks([0, 1], ['Incorrect', 'Correct'])\n",
    "for i, p in enumerate(plt.gca().patches):\n",
    "    height = p.get_height()\n",
    "    plt.text(p.get_x() + p.get_width()/2., height + 5,\n",
    "             f'{height} ({height/len(prediction_df)*100:.1f}%)', ha=\"center\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze prediction probabilities\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Prediction probability distributions\n",
    "plt.subplot(3, 1, 1)\n",
    "sns.kdeplot(data=prediction_df[prediction_df['Actual'] == 0]['Prob_Low'], \n",
    "           label='Correct Low', color='green', shade=True)\n",
    "sns.kdeplot(data=prediction_df[prediction_df['Actual'] != 0]['Prob_Low'], \n",
    "           label='Incorrect Low', color='red', shade=True)\n",
    "plt.title('Probability Distribution for Low Congestion Predictions')\n",
    "plt.xlabel('Probability')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "sns.kdeplot(data=prediction_df[prediction_df['Actual'] == 1]['Prob_Medium'], \n",
    "           label='Correct Medium', color='green', shade=True)\n",
    "sns.kdeplot(data=prediction_df[prediction_df['Actual'] != 1]['Prob_Medium'], \n",
    "           label='Incorrect Medium', color='red', shade=True)\n",
    "plt.title('Probability Distribution for Medium Congestion Predictions')\n",
    "plt.xlabel('Probability')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "sns.kdeplot(data=prediction_df[prediction_df['Actual'] == 2]['Prob_High'], \n",
    "           label='Correct High', color='green', shade=True)\n",
    "sns.kdeplot(data=prediction_df[prediction_df['Actual'] != 2]['Prob_High'], \n",
    "           label='Incorrect High', color='red', shade=True)\n",
    "plt.title('Probability Distribution for High Congestion Predictions')\n",
    "plt.xlabel('Probability')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get misclassified instances\n",
    "def analyze_misclassifications(prediction_df, X_test, feature_names):\n",
    "    # Get misclassified instances\n",
    "    misclassified = prediction_df[~prediction_df['Correct']].copy()\n",
    "    \n",
    "    # Add feature values to misclassified DataFrame\n",
    "    misclassified_features = pd.DataFrame(X_test[~prediction_df['Correct']], \n",
    "                                          columns=feature_names)\n",
    "    \n",
    "    misclassified_data = pd.concat([misclassified.reset_index(drop=True), \n",
    "                                    misclassified_features.reset_index(drop=True)], \n",
    "                                   axis=1)\n",
    "    \n",
    "    return misclassified_data\n",
    "\n",
    "# Analyze misclassifications\n",
    "misclassified_data = analyze_misclassifications(prediction_df, X_test_scaled, X_reduced.columns)\n",
    "\n",
    "# Display summary of misclassifications\n",
    "print(f\"Total misclassifications: {len(misclassified_data)} out of {len(prediction_df)} ({len(misclassified_data)/len(prediction_df)*100:.2f}%)\\n\")\n",
    "\n",
    "# Group misclassifications by actual and predicted classes\n",
    "misclass_summary = pd.crosstab(misclassified_data['Actual'], misclassified_data['Predicted'],\n",
    "                               rownames=['Actual'], colnames=['Predicted'])\n",
    "print(\"Misclassification summary:\")\n",
    "misclass_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize misclassification patterns\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "misclass_summary.plot(kind='bar', stacked=True, colormap='viridis')\n",
    "plt.title('Misclassification Patterns')\n",
    "plt.xlabel('Actual Congestion Level')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks([0, 1, 2], ['Low', 'Medium', 'High'])\n",
    "plt.legend(title='Predicted as')\n",
    "\n",
    "# Convert to percentages for clearer visualization\n",
    "misclass_percent = misclass_summary.div(misclass_summary.sum(axis=1), axis=0) * 100\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "misclass_percent.plot(kind='bar', stacked=True, colormap='viridis')\n",
    "plt.title('Misclassification Percentages')\n",
    "plt.xlabel('Actual Congestion Level')\n",
    "plt.ylabel('Percentage')\n",
    "plt.xticks([0, 1, 2], ['Low', 'Medium', 'High'])\n",
    "plt.legend(title='Predicted as')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Model Application: Predicting Congestion Level for New Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Predicting congestion level for synthetic test cases\n",
    "# Let's create some sample data points with different characteristics\n",
    "\n",
    "# Create a function to prepare new data for prediction\n",
    "def prepare_data_for_prediction(new_data, scaler):\n",
    "    # Extract required features\n",
    "    required_features = X_reduced.columns\n",
    "    \n",
    "    # Ensure all required features are present\n",
    "    for feature in required_features:\n",
    "        if feature not in new_data.columns:\n",
    "            raise ValueError(f\"Feature '{feature}' is missing from the input data\")\n",
    "    \n",
    "    # Select only the required features and in the correct order\n",
    "    X_new = new_data[required_features]\n",
    "    \n",
    "    # Scale the features\n",
    "    X_new_scaled = scaler.transform(X_new)\n",
    "    \n",
    "    return X_new_scaled\n",
    "\n",
    "# Create sample test cases\n",
    "test_cases = [\n",
    "    {\"Packet_Loss_Rate\": 0.01, \"Average_Latency_ms\": 10.0, \"Node_Betweenness_Centrality\": 0.5, \n",
    "     \"Traffic_Volume_MBps\": 100.0, \"Link_Stability_Score\": 0.9, \"Hour\": 12, \"Day\": 15, \"DayOfWeek\": 2},  # Expected: Low\n",
    "    \n",
    "    {\"Packet_Loss_Rate\": 0.08, \"Average_Latency_ms\": 120.0, \"Node_Betweenness_Centrality\": 0.6, \n",
    "     \"Traffic_Volume_MBps\": 500.0, \"Link_Stability_Score\": 0.7, \"Hour\": 18, \"Day\": 20, \"DayOfWeek\": 4},  # Expected: Medium\n",
    "    \n",
    "    {\"Packet_Loss_Rate\": 0.19, \"Average_Latency_ms\": 250.0, \"Node_Betweenness_Centrality\": 0.8, \n",
    "     \"Traffic_Volume_MBps\": 900.0, \"Link_Stability_Score\": 0.5, \"Hour\": 8, \"Day\": 25, \"DayOfWeek\": 1}   # Expected: High\n",
    "]\n",
    "\n",
    "# Create additional derived features (if they were in the model)\n",
    "for case in test_cases:\n",
    "    if 'Loss_Latency_Interaction' in X_reduced.columns:\n",
    "        case['Loss_Latency_Interaction'] = case['Packet_Loss_Rate'] * case['Average_Latency_ms']\n",
    "    if 'Traffic_Stability_Ratio' in X_reduced.columns:\n",
    "        case['Traffic_Stability_Ratio'] = case['Traffic_Volume_MBps'] / case['Link_Stability_Score']\n",
    "    if 'Centrality_Loss_Interaction' in X_reduced.columns:\n",
    "        case['Centrality_Loss_Interaction'] = case['Node_Betweenness_Centrality'] * case['Packet_Loss_Rate']\n",
    "    if 'IsWeekend' in X_reduced.columns:\n",
    "        case['IsWeekend'] = 1 if case['DayOfWeek'] >= 5 else 0\n",
    "\n",
    "# Convert to DataFrame\n",
    "test_cases_df = pd.DataFrame(test_cases)\n",
    "\n",
    "# Prepare data for prediction\n",
    "X_test_cases_scaled = prepare_data_for_prediction(test_cases_df, scaler)\n",
    "\n",
    "# Predict congestion levels\n",
    "predictions = best_model.predict(X_test_cases_scaled)\n",
    "probabilities = best_model.predict_proba(X_test_cases_scaled)\n",
    "\n",
    "# Create a DataFrame with predictions\n",
    "results_df = test_cases_df.copy()\n",
    "results_df['Predicted_Level_Numeric'] = predictions\n",
    "results_df['Predicted_Level'] = ['Low', 'Medium', 'High'][predictions]\n",
    "results_df['Probability_Low'] = probabilities[:, 0]\n",
    "results_df['Probability_Medium'] = probabilities[:, 1]\n",
    "results_df['Probability_High'] = probabilities[:, 2]\n",
    "\n",
    "# Display results\n",
    "print(f\"Predictions using {best_model_name}:\")\n",
    "results_df[['Packet_Loss_Rate', 'Average_Latency_ms', 'Traffic_Volume_MBps', \n",
    "           'Predicted_Level', 'Probability_Low', 'Probability_Medium', 'Probability_High']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the prediction probabilities\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "x = np.arange(len(results_df))\n",
    "width = 0.25\n",
    "\n",
    "plt.bar(x - width, results_df['Probability_Low'], width, label='Low', color='green', alpha=0.7)\n",
    "plt.bar(x, results_df['Probability_Medium'], width, label='Medium', color='orange', alpha=0.7)\n",
    "plt.bar(x + width, results_df['Probability_High'], width, label='High', color='red', alpha=0.7)\n",
    "\n",
    "plt.xlabel('Test Case')\n",
    "plt.ylabel('Probability')\n",
    "plt.title('Prediction Probabilities for Test Cases')\n",
    "plt.xticks(x, [f'Case {i+1}' for i in range(len(results_df))])\n",
    "plt.legend()\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "for i, p in enumerate(results_df['Predicted_Level']):\n",
    "    plt.text(i, 1.05, f\"Predicted: {p}\", ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Conclusion and Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of Findings\n",
    "\n",
    "In this analysis, we have:\n",
    "\n",
    "1. **Preprocessed the network congestion dataset** by handling outliers, creating new features, and preparing the data for modeling.\n",
    "\n",
    "2. **Performed exploratory data analysis** to understand patterns in network congestion, including temporal patterns and regional variations.\n",
    "\n",
    "3. **Engineered features** such as time-based attributes and interaction terms to enhance model performance.\n",
    "\n",
    "4. **Trained multiple machine learning models** including Random Forest, XGBoost, LightGBM, and others.\n",
    "\n",
    "5. **Evaluated model performance** using various metrics, with the best model achieving high accuracy and F1 score.\n",
    "\n",
    "6. **Analyzed feature importance** to understand key drivers of network congestion.\n",
    "\n",
    "7. **Examined model predictions** and analyzed misclassifications to identify patterns.\n",
    "\n",
    "8. **Applied the model** to predict congestion levels for new data.\n",
    "\n",
    "### Key Insights\n",
    "\n",
    "1. The most important factors for predicting network congestion include packet loss rate, latency, and traffic volume.\n",
    "\n",
    "2. There are distinct patterns in congestion based on time of day and day of week.\n",
    "\n",
    "3. Some connection pairs consistently experience higher congestion than others.\n",
    "\n",
    "4. Regional variations in congestion suggest that infrastructure quality or demand patterns differ across regions.\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Model Deployment**: Deploy the best model in a production environment for real-time congestion prediction.\n",
    "\n",
    "2. **Continuous Learning**: Implement a feedback loop to continuously improve the model with new data.\n",
    "\n",
    "3. **Additional Features**: Consider incorporating additional features such as weather data, special events, or hardware specifications.\n",
    "\n",
    "4. **Predictive Maintenance**: Use the model to identify network segments at risk of congestion before issues occur.\n",
    "\n",
    "5. **Time Series Analysis**: Implement more sophisticated time series models to better capture temporal patterns.\n",
    "\n",
    "6. **User Interface**: Develop a dashboard for network administrators to visualize predictions and take preventive actions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
